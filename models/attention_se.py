""" 
ì½”ë“œ1
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from timm.models.layers import DropPath


# âœ… VGG-16ì„ í™œìš©í•œ Feature Extractor
class VGG16FeatureExtractor(nn.Module):
    def __init__(self):
        super(VGG16FeatureExtractor, self).__init__()
        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features

        self.conv1_2 = nn.Sequential(*vgg16[:4])  # Conv1_2
        self.conv2_2 = nn.Sequential(*vgg16[4:9])  # Conv2_2
        self.conv3_3 = nn.Sequential(*vgg16[9:16])  # Conv3_3
        self.conv4_3 = nn.Sequential(*vgg16[16:23])  # Conv4_3
        self.conv5_3 = nn.Sequential(*vgg16[23:30])  # Conv5_3

    def forward(self, x):
        feat1 = self.conv1_2(x)
        feat2 = self.conv2_2(feat1)
        feat3 = self.conv3_3(feat2)
        feat4 = self.conv4_3(feat3)
        feat5 = self.conv5_3(feat4)

        return feat1, feat2, feat3, feat4, feat5


# âœ… Context-aware Pyramid Feature Extraction (CPFE)
class CPFE(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(CPFE, self).__init__()

        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv3x3_r3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3, dilation=3)
        self.conv3x3_r5 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=5, dilation=5)
        self.conv3x3_r7 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=7, dilation=7)

        self.fuse_conv = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1)

    def forward(self, x):
        feat1 = self.conv1x1(x)
        feat2 = self.conv3x3_r3(x)
        feat3 = self.conv3x3_r5(x)
        feat4 = self.conv3x3_r7(x)

        fused = torch.cat([feat1, feat2, feat3, feat4], dim=1)
        return self.fuse_conv(fused)


# âœ… CBAM (Convolutional Block Attention Module)
class CBAM(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super(CBAM, self).__init__()

        # Channel Attention
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.mlp = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction),
            nn.ReLU(),
            nn.Linear(in_channels // reduction, in_channels)
        )

        # Spatial Attention
        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)

    def forward(self, x):
        b, c, h, w = x.size()

        # Channel Attention
        avg_out = self.mlp(self.avg_pool(x).view(b, c)).view(b, c, 1, 1)
        max_out = self.mlp(self.max_pool(x).view(b, c)).view(b, c, 1, 1)
        channel_attn = torch.sigmoid(avg_out + max_out)
        x = x * channel_attn

        # Spatial Attention
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        spatial_attn = torch.sigmoid(self.spatial_conv(torch.cat([avg_out, max_out], dim=1)))

        return x * spatial_attn


# âœ… Hard Negative Cross Attention (HNCA)
class HardNegativeCrossAttention(nn.Module):
    def __init__(self, in_dim):
        super(HardNegativeCrossAttention, self).__init__()
        self.in_dim = in_dim
        self.query = nn.Linear(in_dim, in_dim)
        self.key = nn.Linear(in_dim, in_dim)
        self.value = nn.Linear(in_dim, in_dim)
        self.output_proj = nn.Conv2d(in_dim, in_dim, kernel_size=1)

    def forward(self, x):
        b, c, h, w = x.shape
        x_flat = x.view(b, c, -1).permute(0, 2, 1)
        Q = self.query(x_flat)
        K = self.key(x_flat)
        V = self.value(x_flat)

        attn_scores = torch.softmax(Q @ K.transpose(-2, -1) / (self.in_dim ** 0.5), dim=-1)
        attn_output = attn_scores @ V
        attn_output = attn_output.permute(0, 2, 1).view(b, c, h, w)

        return self.output_proj(attn_output + x)


# âœ… ìµœì¢… ëª¨ë¸
class EnhancedDistortionDetectionModel(nn.Module):
    def __init__(self):
        super(EnhancedDistortionDetectionModel, self).__init__()

        self.vgg = VGG16FeatureExtractor()
        self.cbam = CBAM(64)
        self.cpfe = CPFE(512, 64)
        self.hnca = HardNegativeCrossAttention(64)

        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)
        self.final_conv = nn.Conv2d(128, 1, kernel_size=3, padding=1)

    def forward(self, x):
        feat1, feat2, feat3, feat4, feat5 = self.vgg(x)
        low_feat = self.cbam(feat1) * feat1
        high_feat = self.hnca(self.cpfe(feat5))

        high_feat = self.upsample(high_feat)
        fused_feat = torch.cat([low_feat, high_feat], dim=1)
        output = self.final_conv(fused_feat)

        return output.view(output.shape[0], -1).mean(dim=1)


# âœ… ì†ì‹¤ í•¨ìˆ˜
def distortion_loss(pred, gt):
    mse_loss = nn.MSELoss()(pred, gt)
    perceptual_loss = torch.mean(torch.abs(pred - gt))
    return mse_loss + 0.1 * perceptual_loss


# âœ… í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    dummy_input = torch.randn(2, 3, 224, 224)  # âœ… ì…ë ¥ í¬ê¸° 224x224ë¡œ ì„¤ì •
    dummy_gt = torch.randn(2)  # âœ… MOS ì ìˆ˜ (batch_size,) í˜•íƒœë¡œ ìƒì„±
    model = EnhancedDistortionDetectionModel()
    output = model(dummy_input)

    loss = distortion_loss(output, dummy_gt)

    print("Model Output Shape:", output.shape)  # âœ… (batch_size,)ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•¨
    print("Loss:", loss.item())
 """


""" 
ì½”ë“œ2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from timm.models.layers import DropPath


# âœ… VGG-16ì„ í™œìš©í•œ Feature Extractor
class VGG16FeatureExtractor(nn.Module):
    def __init__(self):
        super(VGG16FeatureExtractor, self).__init__()
        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features

        self.conv1_2 = nn.Sequential(*vgg16[:4])  # Conv1_2
        self.conv2_2 = nn.Sequential(*vgg16[4:9])  # Conv2_2
        self.conv3_3 = nn.Sequential(*vgg16[9:16])  # Conv3_3
        self.conv4_3 = nn.Sequential(*vgg16[16:23])  # Conv4_3
        self.conv5_3 = nn.Sequential(*vgg16[23:30])  # Conv5_3

    def forward(self, x):
        feat1 = self.conv1_2(x)
        feat2 = self.conv2_2(feat1)
        feat3 = self.conv3_3(feat2)
        feat4 = self.conv4_3(feat3)
        feat5 = self.conv5_3(feat4)

        return feat1, feat2, feat3, feat4, feat5


# âœ… Multi-Scale Feature Aggregation (MSFA)
class MultiScaleFeatureAggregation(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(MultiScaleFeatureAggregation, self).__init__()
        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv5x5 = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2)
        self.fuse_conv = nn.Conv2d(out_channels * 3, out_channels, kernel_size=1)

    def forward(self, x):
        feat1 = self.conv1x1(x)
        feat2 = self.conv3x3(x)
        feat3 = self.conv5x5(x)
        fused = torch.cat([feat1, feat2, feat3], dim=1)
        return self.fuse_conv(fused)


# âœ… CoordAttention (Coordinate Attention)
class CoordAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super(CoordAttention, self).__init__()
        self.avg_pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.avg_pool_w = nn.AdaptiveAvgPool2d((1, None))
        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(in_channels // reduction)
        self.conv_h = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)
        self.conv_w = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)

    def forward(self, x):
        B, C, H, W = x.shape
        h_attn = self.avg_pool_h(x).permute(0, 1, 3, 2)
        w_attn = self.avg_pool_w(x)
        shared_feat = torch.cat([h_attn, w_attn], dim=2)
        shared_feat = self.conv1(shared_feat)
        shared_feat = self.bn(shared_feat)
        shared_feat = F.relu(shared_feat)
        split_size = shared_feat.shape[2] // 2
        h_attn, w_attn = torch.split(shared_feat, [split_size, split_size], dim=2)
        h_attn = self.conv_h(h_attn.permute(0, 1, 3, 2))
        w_attn = self.conv_w(w_attn)
        attn = torch.sigmoid(h_attn + w_attn)
        return x * attn


# âœ… Context-aware Pyramid Feature Extraction (CPFE)
class CPFE(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(CPFE, self).__init__()
        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv3x3_r3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3, dilation=3)
        self.conv3x3_r5 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=5, dilation=5)
        self.conv3x3_r7 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=7, dilation=7)
        self.fuse_conv = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1)

    def forward(self, x):
        feat1 = self.conv1x1(x)
        feat2 = self.conv3x3_r3(x)
        feat3 = self.conv3x3_r5(x)
        feat4 = self.conv3x3_r7(x)
        fused = torch.cat([feat1, feat2, feat3, feat4], dim=1)
        return self.fuse_conv(fused)


# âœ… Hard Negative Cross Attention (HNCA)
class HardNegativeCrossAttention(nn.Module):
    def __init__(self, in_dim):
        super(HardNegativeCrossAttention, self).__init__()
        self.query = nn.Linear(in_dim, in_dim)
        self.key = nn.Linear(in_dim, in_dim)
        self.value = nn.Linear(in_dim, in_dim)
        self.output_proj = nn.Conv2d(in_dim, in_dim, kernel_size=1)

    def forward(self, x):
        b, c, h, w = x.shape
        x_flat = x.view(b, c, -1).permute(0, 2, 1)
        Q = self.query(x_flat)
        K = self.key(x_flat)
        V = self.value(x_flat)
        attn_scores = torch.softmax(Q @ K.transpose(-2, -1) / (self.in_dim ** 0.5), dim=-1)
        attn_output = attn_scores @ V
        attn_output = attn_output.permute(0, 2, 1).view(b, c, h, w)
        return self.output_proj(attn_output + x)


# âœ… ìµœì¢… ëª¨ë¸
class EnhancedDistortionDetectionModel(nn.Module):
    def __init__(self):
        super(EnhancedDistortionDetectionModel, self).__init__()
        self.vgg = VGG16FeatureExtractor()
        self.msfa = MultiScaleFeatureAggregation(64, 64)
        self.coord_attn = CoordAttention(64)
        self.cpfe = CPFE(512, 64)
        self.hnca = HardNegativeCrossAttention(64)
        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)
        self.final_conv = nn.Conv2d(128, 1, kernel_size=3, padding=1)

    def forward(self, x):
        feat1, feat2, feat3, feat4, feat5 = self.vgg(x)
        low_feat = self.msfa(feat1)
        low_feat = self.coord_attn(low_feat) * low_feat
        high_feat = self.hnca(self.cpfe(feat5))
        high_feat = self.upsample(high_feat)
        fused_feat = torch.cat([low_feat, high_feat], dim=1)
        output = self.final_conv(fused_feat)
        return output.view(output.shape[0], -1).mean(dim=1)


# âœ… ì†ì‹¤ í•¨ìˆ˜
def distortion_loss(pred, gt):
    mse_loss = nn.MSELoss()(pred, gt)
    perceptual_loss = torch.mean(torch.abs(pred - gt))
    return mse_loss + 0.1 * perceptual_loss


# âœ… í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    dummy_input = torch.randn(2, 3, 224, 224)  # âœ… ì…ë ¥ í¬ê¸° 224x224ë¡œ ì„¤ì •
    dummy_gt = torch.randn(2)  # âœ… MOS ì ìˆ˜ (batch_size,) í˜•íƒœë¡œ ìƒì„±
    model = EnhancedDistortionDetectionModel()
    output = model(dummy_input)
    loss = distortion_loss(output, dummy_gt)

    print("Model Output Shape:", output.shape)  # âœ… (batch_size,)ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•¨
    print("Loss:", loss.item())  # âœ… ì†ì‹¤ ê°’ ì¶œë ¥
 """



import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from timm.models.layers import DropPath


# âœ… VGG-16ì„ í™œìš©í•œ Feature Extractor
class VGG16FeatureExtractor(nn.Module):
    def __init__(self):
        super(VGG16FeatureExtractor, self).__init__()
        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features

        self.conv1_2 = nn.Sequential(*vgg16[:4])  # Conv1_2
        self.conv2_2 = nn.Sequential(*vgg16[4:9])  # Conv2_2
        self.conv3_3 = nn.Sequential(*vgg16[9:16])  # Conv3_3
        self.conv4_3 = nn.Sequential(*vgg16[16:23])  # Conv4_3
        self.conv5_3 = nn.Sequential(*vgg16[23:30])  # Conv5_3

    def forward(self, x):
        feat1 = self.conv1_2(x)
        feat2 = self.conv2_2(feat1)
        feat3 = self.conv3_3(feat2)
        feat4 = self.conv4_3(feat3)
        feat5 = self.conv5_3(feat4)

        return feat1, feat2, feat3, feat4, feat5


# âœ… Context-aware Pyramid Feature Extraction (CPFE)
class CPFE(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(CPFE, self).__init__()

        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv3x3_r3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3, dilation=3)
        self.conv3x3_r5 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=5, dilation=5)
        self.conv3x3_r7 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=7, dilation=7)

        self.fuse_conv = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1)

    def forward(self, x):
        feat1 = self.conv1x1(x)
        feat2 = self.conv3x3_r3(x)
        feat3 = self.conv3x3_r5(x)
        feat4 = self.conv3x3_r7(x)

        fused = torch.cat([feat1, feat2, feat3, feat4], dim=1)
        return self.fuse_conv(fused)


# âœ… CoordAttention (Coordinate Attention) - 2021
class CoordAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super(CoordAttention, self).__init__()

        self.avg_pool_h = nn.AdaptiveAvgPool2d((None, 1))  # Height ë°©í–¥
        self.avg_pool_w = nn.AdaptiveAvgPool2d((1, None))  # Width ë°©í–¥

        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, stride=1, bias=False)
        self.bn = nn.BatchNorm2d(in_channels // reduction)

        self.conv_h = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, stride=1, bias=False)
        self.conv_w = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, stride=1, bias=False)

    def forward(self, x):
        B, C, H, W = x.shape

        # âœ… Height ë°©í–¥ê³¼ Width ë°©í–¥ í‰ê·  í’€ë§
        h_attn = self.avg_pool_h(x).permute(0, 1, 3, 2)  # (B, C, 1, H) â†’ (B, C, H, 1)
        w_attn = self.avg_pool_w(x)  # (B, C, 1, W)

        # âœ… ê³µìœ ëœ Convolution ì ìš©
        shared_feat = torch.cat([h_attn, w_attn], dim=2)  # (B, C, H+W, 1)
        shared_feat = self.conv1(shared_feat)
        shared_feat = self.bn(shared_feat)
        shared_feat = F.relu(shared_feat)

        # âœ… Height / Width ë°©í–¥ ë¶„ë¦¬ í›„ ê°ê° Attention ì ìš© (ìë™ split)
        split_size = shared_feat.shape[2] // 2  # ğŸ”§ ìë™ ê³„ì‚°ëœ í¬ê¸°ë¡œ split
        h_attn, w_attn = torch.split(shared_feat, [split_size, split_size], dim=2)  # âœ… ì˜¤ë¥˜ í•´ê²°

        h_attn = self.conv_h(h_attn.permute(0, 1, 3, 2))  # (B, C, H, 1) â†’ (B, C, 1, H)
        w_attn = self.conv_w(w_attn)  # (B, C, 1, W)

        attn = torch.sigmoid(h_attn + w_attn)  # ìµœì¢… Attention Map
        return x * attn  # ì…ë ¥ Featureì— Attention ì ìš©

# âœ… Hard Negative Cross Attention (HNCA)
class HardNegativeCrossAttention(nn.Module):
    def __init__(self, in_dim, num_negatives=5):  # âœ… `num_negatives` ì¶”ê°€
        super(HardNegativeCrossAttention, self).__init__()
        self.in_dim = in_dim
        self.num_negatives = num_negatives  # âœ… ì €ì¥

        self.query = nn.Linear(in_dim, in_dim)
        self.key = nn.Linear(in_dim, in_dim)
        self.value = nn.Linear(in_dim, in_dim)
        self.output_proj = nn.Conv2d(in_dim, in_dim, kernel_size=1)

        self.channel_fix = nn.Conv2d(3, in_dim, kernel_size=1)  # âœ… ì±„ë„ ë³€í™˜ ì¶”ê°€

    def forward(self, x, neg_x):
        b, c, h, w = x.shape
        neg_b, neg_c, neg_h, neg_w = neg_x.shape

        # âœ… Batch í¬ê¸° ë§ì¶”ê¸° (xì™€ ë™ì¼í•œ Batch í¬ê¸°ë¡œ ì¡°ì •)
        if neg_b != b:
            neg_x = neg_x[:b]  # âœ… Batch í¬ê¸°ë¥¼ ë§ì¶°ì£¼ê¸°

        # âœ… í¬ê¸° ë³€í™˜ (neg_x: 3 â†’ 64)
        neg_x = self.channel_fix(neg_x)

        # âœ… í¬ê¸° ë§ì¶”ê¸° (xì™€ ë™ì¼í•œ H, W í¬ê¸°ë¡œ ë³€í™˜)
        neg_x = F.interpolate(neg_x, size=(h, w), mode="bilinear", align_corners=False)

        # âœ… view ì—°ì‚° ì‹œ í¬ê¸° ì¼ì¹˜í•˜ë„ë¡ ì¡°ì •
        neg_x_flat = neg_x.view(b, c, -1).permute(0, 2, 1)  # ğŸ”§ ìˆ˜ì •

        Q = self.query(x.view(b, c, -1).permute(0, 2, 1))
        K_neg = self.key(neg_x_flat)
        V_neg = self.value(neg_x_flat)

        attn_scores = torch.softmax(Q @ K_neg.transpose(-2, -1) / (self.in_dim ** 0.5), dim=-1)
        attn_output = attn_scores @ V_neg
        attn_output = attn_output.permute(0, 2, 1).view(b, c, h, w)

        return self.output_proj(attn_output + x)


# âœ… ìµœì¢… ëª¨ë¸
class EnhancedDistortionDetectionModel(nn.Module):
    def __init__(self, num_negatives=5):
        super(EnhancedDistortionDetectionModel, self).__init__()

        self.vgg = VGG16FeatureExtractor()
        self.coord_attn = CoordAttention(64)
        self.cpfe = CPFE(512, 64)
        self.hnca = HardNegativeCrossAttention(64, num_negatives)  # âœ… ì´ì œ ì •ìƒ ì‘ë™

        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)
        self.final_conv = nn.Conv2d(128, 1, kernel_size=3, padding=1)

    def forward(self, x, hard_negatives):
        feat1, feat2, feat3, feat4, feat5 = self.vgg(x)
        low_feat = self.coord_attn(feat1) * feat1
        high_feat = self.hnca(self.cpfe(feat5), hard_negatives)

        high_feat = self.upsample(high_feat)
        fused_feat = torch.cat([low_feat, high_feat], dim=1)
        output = self.final_conv(fused_feat)

        return output.view(output.shape[0], -1).mean(dim=1)





# âœ… ì†ì‹¤ í•¨ìˆ˜
def distortion_loss(pred, gt):
    mse_loss = nn.MSELoss()(pred, gt)
    perceptual_loss = torch.mean(torch.abs(pred - gt))
    return mse_loss + 0.1 * perceptual_loss


# âœ… í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    dummy_input = torch.randn(2, 3, 224, 224)  # âœ… ì…ë ¥ í¬ê¸° 224x224ë¡œ ì„¤ì •
    dummy_gt = torch.randn(2)  # âœ… MOS ì ìˆ˜ (batch_size,) í˜•íƒœë¡œ ìƒì„±
    model = EnhancedDistortionDetectionModel()
    output = model(dummy_input)

    loss = distortion_loss(output, dummy_gt)

    print("Model Output Shape:", output.shape)  # âœ… (batch_size,)ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•¨
    print("Loss:", loss.item())